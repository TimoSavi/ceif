## Tweaking and testing ceif

Here are some examples generated by running ceif. Examples are 2-dimensional datasets presented in anomaly score maps. 
Examples show how some ceif parameters can be used to change the behaviour of forest training.

### Data samples

Three different datasets are used in this document: two blobs, square and circle:

|Two blobs|Square|Circle|
|---|---|---|
|![](pics/2blob.png)|![](pics/square.png)|![](pics/circle.png)|

### Used commands

Anomaly maps are generated by running following set of commands (here is the circle as an example). Commands are run in [test](../test) directory:

    ceif -l circle.csv -a test_matrix.csv -O0.0 -p "%v,0x%x,%s" -o plot_data.csv -R 25
    ./make_contour.sh
    gnuplot  -e "datafile='circle.csv'" plot.gp

Now you have the anomaly score map in file 'pic.png'. Commands explained:

* ceif: Run ceif by using file circle.csv as training data. Analyzed file test_matrix.csv contains 160000 data points on x/y range [0,40] with 0.1 resolution. All points
having anomaly score larger or equal than 0 are written to file plot_data.csv using "%v,0x%x,%s" as printing format. See manual for more about ceif parameters.
* make_contour.sh: Read plot_data.csv and separate rows having anomaly score 0.5,0.55,0.6,0.7 and 0.8 (within &#177;0.01 range) each to different file in order to create contour lines having
respective colours blue,brown,green,yellow and red.
* gnuplot: generate score map using plot_data.csv, original training data (in parameter 'datafile') and contour lines

Score maps have colors mapped for anomaly scores 0 -> 0.5 -> 1.0 as respective colors are blue -> green -> red and mixed colors between them.
    
### Results

#### Extended isolation forest (almost)

Running test sets without interception point ***p*** movement around a randomly selected sample point. This is actually good example only for square type of data set, so only it is presented here:

![](pics/square_R0.png)

#### Extended isolation forest with revised algorithm
Following table presents the effect of different '-R' parameter values. Parameter '-R' is used to control how wide the interception ***p*** selection range expands from the sample data points. Default value is one, which causes quite small expansion. Here values 5,15,50 and 150 are tested with three different datasets.


|-R value|Two blobs|Square|Circle|
|---|---|---|---|
|0|![](pics/2blob_R0.png)|![](pics/square_R0.png)|![](pics/circle_R0.png)|
|25|![](pics/2blob_R25.png)|![](pics/square_R25.png)|![](pics/circle_R25.png)|
|75|![](pics/2blob_R75.png)|![](pics/square_R75.png)|![](pics/circle_R75.png)|
|200|![](pics/2blob_R200.png)|![](pics/square_R200.png)|![](pics/circle_R200.png)|

Following effects are seen:

* '-R' values 0 and 25: Contour lines are quite irregular. Especially the circle hole is not clearly identified as outlier area
* '-R' value 75: contour lines are quite regular. Green line (score value 0.6) could be used as anomaly score if it is assumed that training data does not contain anomalies. Blue or brown (score values 0.5 and 0.55) could be used if training data contains anomalies.
* '-R' value 200: When larger '-R' value are used the score lines are expanded but also the clearly outlier area (reddish) is more close to data map. Meaning when the '-R' value is increased the visible anomaly score slope from 0 to 1 is more steep and visible score value range is more wide (e.g. the blueish area in the middle of square). Also the middle of circle is now clearly outlier area. Also contour lines are following the datasets more closely (see two blobs). So larger '-R' value can be used if data set has meandering contours.

It is also noticeable that circle like maps are quite difficult to get right. In above examples only '-R' value 75 (or little less) is usable if the middle of the circle should be seen as outlier area using score 0.5. This due to the nature of the algorithm, it creates sub spaces spanning over inlier and outlier areas causing bias. 

Low -R values can be beneficial when categorizing data. Lower score values are then concentrated to the center of data map. This can yield better results when different forests (categories) are
overlapping.

#### Tricky data maps
Here is an example of a difficult data map. Two nested circles causes problems because there are adjacent inlier and outlier areas and subareas of the algorithm tend to span over both areas. 
Table below has three examples of double circle with different '-R' values. Contour lines are not used for clarity.

| Case | Two circles |
|---|---|
|Data map|![](pics/2circle.png)|
|-R 75|![](pics/2circle_R75.png)|
|-R 150|![](pics/2circle_R150.png)|
|-R 200|![](pics/2circle_R200.png)|

It is hard to get two outlier circles inside right. There are no clear boundaries, selecting outlier score is not a simple task. Also the inner circle tends to get more higher inlier score (bluish in last picture).

### Saving and updating forest data
The result of training phase can be saved to file to be used in later analysis. Data can also be updated with new training data. 
In following example a square map is updated with a small blob:

| Case | File | Map |
|---|---|---|
|Square|square.csv|![](pics/square.png)|
|Small blob|sblob.csv|![](pics/sblob.png)|

Training and saving the square to file square.f:

    ceif -l square.csv -w square.f -R50

Updating the square.f with small blob:

    ceif -r square.f -l sblob.csv -w square.f

Creating anomaly map with commads:

    ceif  -r square.f -a test_matrix.csv -O0.0 -p "%v,0x%x,%s" -o plot_data.csv
    ./make_contour.sh
    gnuplot  -e "datafile='square.csv'" plot.gp

Resulting map in pic.png:

![](pics/square_sblob.png)

### Performance
Here are some performance test run using covtype.data from [UC Irvine Machine Learning Repository](https://archive.ics.uci.edu/ml/datasets/covertype). File has 55 columns and 581012 rows.
Tests have been run using Intel i5-650 Processor, 3.20 GHz and 8 GB ram.

#### learn with category
First 10 fields are used in analysis and field 55 is used as category field. Forest data is written to file.

    time ceif -l /covtype.data -I1-200 -U1-10 -C55 -R50 -w covtype.f1
    
    real    0m1,189s
    user    0m1,089s
    sys     0m0,045s

#### Analyze using forest data from previous run
Anomaly score 0.9 is used to disable printing.

    time ceif -r covtype.f1 -a covtype.data -O0.9
    
    real    0m16,504s
    user    0m16,442s
    sys     0m0,048s

#### categorizing data
Categorizing takes lot of time because each analyzed row must be run through each forest.

    time ceif -r covtype.f1 -c covtype.data  -p "%c %C %v" -o /dev/null
    
    real    1m52,876s
    user    1m52,340s
    sys     0m0,240s

